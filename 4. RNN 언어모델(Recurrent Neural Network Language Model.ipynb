{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RNN 언어 모델(Recurrent Neural Network Language Model, RNNLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram 언어 모델과 NNLM은 고정된 개수의 단어만을 입력으로 받아야한다는 단점이 있었습니다.\n",
    "하지만 시점(time step)이라는 개념이 도입된 RNN으로 언어 모델을 만들면 입력의 길이를 고정하지 않을 수 있습니다. 이처럼 RNN으로 만든 언어모델을 RNNLM(Recurrent Neural Network Language Model)이라고 합니다.\n",
    "\n",
    "RNNLM이 언어 모델링을 학습하는 과정을 보겠습니다.\n",
    "\n",
    "예문 : 'What will the fat cat sit on'\n",
    "\n",
    "예를 들어 훈련 코퍼스에 위와 같은 문장이 있다고 해봅시다.\n",
    "언어 모델은 주어진 단어 시퀀스로부터 다음 단어를 예측하는 모델입니다. 아래 그림은 RNNLM이 어떻게 이전 시점의 단어들과 현재 시점의 단어로 다음 단어를 예측하는지를 보여줍니다.\n",
    "\n",
    "![Alt text](image-16.png)\n",
    "\n",
    "훈련 과정에서는 이전 시점의 예측 결과를 다음 시점의 입력으로 넣으면서 예측하는 것이 아니라, what will the fat cat sit on라는 훈련 샘플이 있다면, what will the fat cat sit 시퀀스를 모델의 입력으로 넣으면, will the fat cat sit on를 예측하도록 훈련됩니다. will, the, fat, cat, sit, on는 각 시점의 레이블입니다.\n",
    "\n",
    "이러한 훈련 기법을 교사 강요(teacher forcing)라고 합니다. 교사 강요란, 테스트 과정에서 t시점의 출력이 t+1 시점의 입력으로 사용되는 RNN 모델을 훈련시킬 떄 사용하는 훈련 기법입니다.\n",
    "\n",
    "훈련할 때 교사 강요를 사용할 경우, 모델이 t 시점에서 예측한 값을 t+1 시점에 입력으로 사용하지 않고, t 시점의 레이블. 즉 실제 알고있는 정답을 t+1 시점의 입력으로 사용합니다. \n",
    "\n",
    "물론, 훈련 과정에서도 이전 시점의 출력을 다음 시점의 입력으로 사용하면서 훈련 시킬 수도 있지만 이는 한 번 잘못 예측하면 뒤에서의 예측까지 영향을 미쳐 훈련 시간이 느려지게 되므로 교사 강요를 사용하여 RNN을 좀 더 빠르고 효과적으로 훈련시킬 수 있습니다.\n",
    "\n",
    "* 코퍼스(Corpus) : 한국어로는 '말뭉치' 혹은 '말모둠'으로 번역하는, 코퍼스는 글 또는 말 텍스트를 모아 놓은 것이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-17.png)\n",
    "\n",
    "훈련 과정 동안 출력층에서 사용하는 활성화 함수는 소프트맥스 함수입니다.\n",
    "\n",
    "그리고 모델이 예측한 값고 실제 레이블과의 오차를 계산하기 위해서 손실 함수로 크로스 엔트로피 함수를 사용합니다.\n",
    "\n",
    "![Alt text](image-18.png)\n",
    "\n",
    "RNNLM의 구조를 보겠습니다. RNNLM은 위의 그림과 같이 총 4개의 측(layer)으로 이루어진 인공 신경망입니다. 우선 입력층(input layer)을 봅시다. RNNLM의 현 시점(timestep)은 4로 가정합니다. 그래서 4번째 입력 단어인 fat의 원-핫 벡터가 입력이 됩니다.\n",
    "\n",
    "출력층(output layer)을 봅시다. 모델이 예측해야하는 정답에 해당되는 단어 fat의 원-핫 벡터는 출력층에서 모델이 예측한 값의 오차를 구하기 위해서 사용될 예정입니다. 그리고 이 오차로부터 손실 함수를 사용해 인공 신경망이 학습을 하게 됩니다.\n",
    "\n",
    "조금 더 구체적으로 살펴보겠습니다.\n",
    "\n",
    "![Alt text](image-19.png)\n",
    "\n",
    "현 시점의 입력 단어의 원-핫 벡터 xt를 입력받은 RNNLM은 우선 임베딩층(embedding layer)을 지닙니다. 이 임베딩층은 기본적으로 NNLM에서 배운 투사층(projection layer)입니다. NNLM에서는 룩업 테이블을 수행하는 층을 투사층이라고 표현했지만, 이미 투사층의 결과로 얻는 벡터를 임베딩 벡터라고 부른다고 NNLM에서 학습하였으므로, 앞으로는 임베딩 벡터를 얻는 투사층을 임베딩층(embedding layer)이라는 표현을 사용할 겁니다.\n",
    "\n",
    "실제 값과 예측값 두 벡터가 가까워지게 하기위해서 RNNLM는 손실 함수로 cross-entropy 함수를 사용합니다. 그리고 역전파가 이루어지면서 가중치 행렬들이 학습되는데, 이 과정에서 임베딩 벡터들도 학습이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Entropy(번외)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref. https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
