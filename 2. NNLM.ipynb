{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 피드 포워드 신경망 언어모델(Neural Network Language Model, NNLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과거에는 기계가 자연어를 학습하게 하는 방법으로 통계적인 접근을 사용했으나\n",
    "\n",
    "최근에는 인공 신경망을 사용하는 방법이 자연어 처리에서 더 좋은 성능\n",
    "\n",
    "Statical Language Model(SLM)에서 다양한 구조의 인공 신경망을 사용한 언어 모델들로 대체되기 시작.\n",
    "\n",
    "1. 기존 N-gram 언어 모델의 한계\n",
    "언어 모델은 문장에 확률을 할당하는 모델이며, 주어진 문맥으로부터 아직 모르는 단어를 예측하는 것을 언어 모델링이라고함.\n",
    "\n",
    "n-gram 언어 모델은 언어 모델링에 바로 앞 n-1개의 단어를 참고함.\n",
    "\n",
    "![Alt text](image.png)\n",
    "\n",
    "하지만 이러한 n-gram 언어 모델은 충분한 데이터를 관측하지 못하면 언어를 정확히 모델링하지 못하는 희소 문제(sparsity problem)가 있었음.\n",
    "\n",
    "2. 단어의 의미적 유사성\n",
    "희소 문제는 기계가 단어의 의미적 유사성을 알 수 있다면 해결할 수 있는 문제임.\n",
    "예를들어서, '톺아보다'와 '샅샅이 살펴보다'가 있으면\n",
    "\n",
    "n-gram 언어모델은 '살펴보다'와 '톺아보다'의 단어의 유사도를 알 수 없으므로 예측에 고려할 수 없음.\n",
    "\n",
    "만약 언어 모델 또한 단어의 의미적 유사성을 학습할 수 있도록 설계된다면, 훈련 코퍼스에 없는 단어 시퀀스에 대한 예측이라도 유사한 단어가 사용된 단어 시퀀스를 참고하여 보다 정확한 예측 가능.\n",
    "\n",
    "이러한 아이디어를 반영한 언어 모델이 신경망 언어 모델 NNLM임.\n",
    "이 아이디어는 단어 벡터 간 유사도를 수할 수 있는 벡터를 얻어내는 워드 임베딩(word embedding)의 아이디어기도함.\n",
    "\n",
    "3. 피드 포워드 신경망 언어모델(NNLM)\n",
    "예문 : \"what will the fat cat sit on\"\n",
    "\n",
    "훈련 코퍼스가 준비된 상태에서 가장 먼저 해야 할 일은 기계가 단어를 인식할 수 있도록 모든 단어를 수치화하는 것.\n",
    "\n",
    "--> 원 핫 인코딩\n",
    "훈련 코퍼스에 7개의 단어만 존재한다고 가정했을 떄 위 단어들을 원-핫 인코딩 가능\n",
    "모든 단어가 단어 집합의 크기인 7개의 차원을 가지는 원-핫 벡터가 됨.\n",
    "\n",
    "따라서 이 원-핫 벡터들이 훈련을 위한 NNLM의 입력이면서 예측을 위한 레이블이됨.\n",
    "'what will fat cat'를 입력을 받아서 'sit'을 예측하는 일은 기계에게 what, will, the, fat, cat의 원-핫 벡터를 입력받아 sit의 원-핫 벡터를 예측하게 하는 문제임.\n",
    "\n",
    "NNLM은 n-gram 언어 모델처럼 다음 단어를 예측할때, 앞의 모든 단어를 참고하는 것이 아니라 정해진 개수의 단어만을 참고함. 이 개수를 n개라고 한다. 이 범위를 윈도우(window)라고 하기도 함.\n",
    "\n",
    "![Alt text](image-1.png)\n",
    "\n",
    "NNLM의 구조를보자.\n",
    "입력층을 보면 앞에서 윈도우의 크기는 4로 정하였으므로 입력은 'will, the, fat, cat'의 원-핫 벡터임. 출력층을 보면 모델이 예측해야하는 정답에 해당되는 단어 sit의 원 핫 벡터는 모델이 예측한 값의 오차를 구하기 위해 레이블로 사용됨. 그리고 오차로부터 손실 함수를 사용하여 인공 신경망이 학습을 하게됨.\n",
    "\n",
    "![Alt text](image-2.png)\n",
    "\n",
    "원-핫 벡터의 특성으로 인해 i번쨰 인덱스에 1이라는 값을 가지고 그 외의 0의 값을 가지는 원-핫 벡터의 특성으로 인해 i번쨰 인덱스에 1이라는 값을 가지고 그 외의 0의 값을 가지는 원-핫 벡터와 가중치 W 행렬의 곱은 사실 W 행렬의 i번쨰 행을 그대로 읽어오는 것과(lookup) 동일합니다. 그래서 이 작업을 룩업 테이블(lookup table)이라고 합니다.\n",
    "\n",
    "![Alt text](image-3.png)\n",
    "\n",
    "각 안어가 테이블 룩얼을 통해 임베딩 벡터로 변경되고, 투사층에서 모든 임베딩 벡터들의 값은 연결됩니다.(concatenate) 여기서 벡터의 연결 연산은 벡터들을 이어붙이는 것을 의미합니다. 투사층을 식으로 표현하면 다음과 같습니다.\n",
    "\n",
    "![Alt text](image-4.png)\n",
    "\n",
    "일반적인 은닉층이 활성화 함수를 사용하는 비선형층인 것과는 달리 투사층은 활성화 함수가 존재하지 않는 선형층이라는 점이 다소 생소하지만, 이 다음은 다시 은닉층을 사용하는 일반적인 피드 포워드 신경망과 동일합니다.\n",
    "\n",
    "은닉층의 활성화 함수를 하이퍼볼릭탄젠츠 함수라고 하였을때, 은닉층을 식으로 표현하면 다음과 같습니다.\n",
    "\n",
    "![Alt text](image-5.png)\n",
    "\n",
    "출력층에서는 활성화 함수로 소프트맥스(softmax)함수를 사용하는데, V차원의 벡터는 소프트맥스 함수를 지나면서 벡터의 각 원소는 0과 1사이의 실수값을 가지며 총 합은 1이 되는 상태로 바뀜.\n",
    "\n",
    "실제값에 해당되는 다음 단어를 y라고 했을 떄, 이 두 벡터가 가까워지게 하기위해서 NNLM는 손실 함수로 크로스 엔트로피(cross-entropy) 함수를 사용.\n",
    "해당 문제는 단어 집합의 모든 단어라는 V개의 선택지 중 정답인 'sit'을 예측해야하는 다중 클래스 분류문제임. 그리고 역전파가 이루어지면 모든 가중치 행렬들이 학습되는데, 여기서는 투사층에서의 가중치 행렬도 포함되므로 임베딩 벡터값 또한 학습됨.\n",
    "\n",
    "이번 예제에서는 7개의 단어만 사용했지만, 만약 충분한 훈련 데이터가 있다는 가정 하에 NNLM이 얻을 수 있는 이점은?\n",
    "NNLM의 핵심은 충분한 양의 훈련 코퍼스를 위와 같은 과정으로 학습한다면 결과적으로 수많은 문장에서 유사한 목적으로 사용되는 단어들은 결국 유사한 임베딩 벡터값을 얻게됨. 따라서 훈련이 끝난 후 다음 단어를 예측 과정에서 훈련 코퍼스에서 없던 단어 시퀀스라 하더라도 다음 단어를 선택 가능.\n",
    "\n",
    "NNLM은 기존 n-gram 언어 모델의 한계를 개선하였지만 여전히 가지는 문제점이 있음.\n",
    "1) 기존 모델에서의 개선점 : 희소문제 해결\n",
    "2) NNLM이 극복하지 못한 한계 : 정해진 n개의 단어만을 참고할 수 있는점.\n",
    "\n",
    "이 한계를 극복한 언어 모델이 RNN(Recurrent Neural Network)\n",
    "그리고 이를 사용한 RNN 언어 모델(Recurrent Neural Network Language Model, RNNLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 활성화함수(번외)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 활성화 함수의 개념\n",
    "활성화함수(Activation Function)란 퍼셉트론(Perceptron)의 출력값을 결정하는 비선형함수. 즉, 활성화 함수는 퍼셉트론에서 입력값의 총합을 출력할지 말지 결정하고, 출력한다면 어떤 값으로 변환하여 출력할지 결정하는 함수.\n",
    "\n",
    "![Alt text](image-6.png)\n",
    "\n",
    "2. 활성화 함수의 종류\n",
    "1) Sign 함수\n",
    "Sign 함수의 활성화 함수는 퍼셉트론 내 입력값의 총합이 0보다 작을 경우 -1을 출력하고, 반대로 0보다 클 경우 1을 출력하는 역할을 합니다. 이처럼 활성화 함수는 비선형 함수입니다.\n",
    "\n",
    "Sign 함수는 데이터와 결정경계(Decision Boundary)간 거리 정보를 고려하지 않는 한계가 존재합니다. 결정경계와 클래스별로 가장 가까운 데이터 간 거리를 마진(margin)이라고 부르는데, 이 마진의 크기가 클수록 좋은 결정경계입니다.\n",
    "\n",
    "하지만, Sign 함수는 데이터와 결정경계(Desision Boundary) 간 거리 정보를 고려하지 않는다는 한계가 존재합니다. 즉, Sign 함수를 단순히 데이터를 분리했는지 여부에만 관심이 있고 얼마나 잘 분리했는지는 신경쓰지 않습니다.\n",
    "\n",
    "2) Sigmoid 함수\n",
    "모든 입력값에 대해 출력값이 실숫값으로 정의(=Sof Decision)\n",
    "값이 작아질수록 0, 커질수록 1에 수렴\n",
    "출력이 0~1 사이로 호가률 표현 가능(=Binary Classification)\n",
    "Vanishing Gradient 문제 존재\n",
    "\n",
    "3) Tanh 함수\n",
    "모든 입력값에 대해 출력값이 실숫값으로 정의(=Sof Decision)\n",
    "값이 작아질수록 -1, 커질수록 1에 수렴\n",
    "입력값이 0에 가까울수록 미분이 크기때문에 출력값이 빠르게 변함\n",
    "Vanishing Gradient 문제 존재\n",
    "\n",
    "4) Softmax 함수\n",
    "Softmax함수는 N가지 출력값을 갖는 함수로써 입력값을 N가지 클래스 중 하나로 분류하는 Multi-class Classification에 주로 사용됩니다.\n",
    "\n",
    "출력값이 N개\n",
    "입력값을 각각 지수함수로 취하고, 이를 정규화(=총합을 1로 만듦)\n",
    "정규화로 인해 각 출력값은 0~1 값을 가짐\n",
    "모든 출력값의 합은 반드시 1\n",
    "N가지 중 한 가지에 속할 확률 표현 가능 (=Multi-class Classification)\n",
    "\n",
    "* 앞서 다른 Sigmoid 함수는 하나의 입력을 0으로 고정한 2-Class Softmax(0 또는 sigmoid(x)) 함수와 동일합니다.\n",
    "\n",
    "5) ReLU 함수\n",
    "렐루(Rectified Linear Unit, ReLU) 함수는 y = x인 선형함수가 입력값 0 이하에서부터 rectified(정류)된 함수입니다.\n",
    "\n",
    "![Alt text](image-7.png)\n",
    "\n",
    "Sigmoid, tanh 함수의 Vanishing Gradient 문제 해결\n",
    "입력값이 음수일 경우 출력값과 미분값을 모두 0으로 강제하므로 죽은 뉴런을 회생하는 데 어려움 존재(Dying ReLU)\n",
    "구현이 단순하고 연산이 필요 없이 임계값(양수/음수 여부)만 활용하므로 연산 속도 빠름\n",
    "\n",
    "\n",
    "6) Leaky ReLU 함수\n",
    "ReLU 함수에서 발생하는 Dying ReLU 현상을 보완하기 위해 다양한 변형된 ReLU 함수가 제안되었습니다. 그중에서 Leaky ReLU에 대해 알아봅시다. Leaky ReLU는 입력값이 음수일 때 출력값을 0이 아닌 0.001과 같은 매우 작은 값을 출력하도록 설정합니다.\n",
    "\n",
    "수식은 다음과 같습니다.\n",
    "max(ax, x)\n",
    "\n",
    "여기서 a는 0.01, 0.001과 같이 작은 값 중 하나로 하이퍼 파라미터입니다. Leaky는 사전적으로 새어 나가는 같은 의미가 있습니다. a는 0이 아닌 값이기 때문에 입렦밧이 음수라도 기울기가 0이 되지 않아 뉴런이 죽는 현상을 방지할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanishing Gradient(번외)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝 분야에서 Layer를 많이 쌓을수록 데이터 표현력이 증가하기 때문에 학습이 잘 될것 같지만 실제로는 Layer가 많아질수록 학습이 잘 되지 않습니다. 바로 기울기 소실(Vanishing Gradient) 현상 때문입니다.\n",
    "\n",
    "기울기 소실이란 역전파(Backpropagation) 과정에서 출력층에서 멀어질수록 Gradient 값이 매우 작아지는 현상을 말합니다. \n",
    "\n",
    "(역전파 이해하기 : https://wikidocs.net/37406)\n",
    "\n",
    "(기울기 소실 이해하기 : https://heytech.tistory.com/388)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
