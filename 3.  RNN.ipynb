{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순환 신경망(Recurrent Neural Network, RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN(Recurrent Neural Network)은 입력과 출력을 시퀀스 단위로 처리하는 시퀀스(Sequence) 모델입니다.\n",
    "\n",
    "예를들어, 번역기를 생각해보면 입력은 번역하고자 하는 단어의 시퀀스인 문장입니다. 출력에 해당되는 번역된 문장 또한 단어의 시퀀스입니다. 이와 같이 시퀀스들을 처리하기 위해 고안된 모델들을 시퀀스 모델이라고 합니다.\n",
    "\n",
    "LSTM이나 GRU 또한 근본적으로 RNN에 속합니다.\n",
    "\n",
    "** 재귀신경망(Recursive Neural Network)은 용어는 비슷하지만 전혀 다른 개념입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 순환신경망(Recurrent Neural Network, RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "피드 포워드 신경망(Feed Forward Neural Network)는 전부 은닉층에서 활성화 함수를 지닌 값은 오직 출력층 방향으로만 향했습니다.\n",
    "\n",
    "그렇지 않은 신경망들이 있는데, RNN(Recurrent Neural Network)또한 그 중 하나입니다. RNN은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징을 갖고 있습니다\n",
    "\n",
    "RNN에서 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드를 셀(cell)이라고 합니다. 이 셀은 이전의 ㄱ밧을 기억하려고 하는 일종의 메모리 역할을 수행하므로 이를 메모리 셀 또는 RNN 셀이라고 표현합니다.\n",
    "\n",
    "은닉층의 메모리 셀은 각각의 시점(time step)에서 바로 이전 시점에서의 은닉층의 메모리 셀에서 나온 값을 자신의 입력으로 사용하는 재귀적 활동을 하고 있습니다. 현재 시점 t에서의 메모리 셀이 갖고있는 값은 과거의 메모리 셀들의 값에 영향을 받은 것임을 의미합니다.\n",
    "\n",
    "메모리 셀이 출력층 방향 또는 다음 시점인 t+1의 자신에게 보내는 값을 은닉 상태(hidden state)라고 합니다. 다시 말해 t 시점의 메모리 셀은 t-1 시점의 메모리 셀이 보낸 은닉 상태값을 t 시점의 은닉 상태 계산을 위한 입력값으로 사용합니다.\n",
    "\n",
    "![Alt text](image-8.png)\n",
    "\n",
    "피드 포워드 신경망에서는 뉴런이라는 단위를 사용했지만, RNN에서는 뉴런이라는 단위보다는 입력층과 출력층에서는 각각 입력 벡터와 출력 벡터, 은닉층에서는 은닉 상태라는 표현을 주로 사용합니다.\n",
    "\n",
    "RNN은 입력과 출력의 길이를 다르게 설계할 수 있으므로 다양한 용도로 사용할 수 있습니다. 예를들어서 RNN 셀의 각 시점의 입, 출력의 단위는 사용자가 정의하기 나름이지만 가장 보편적인 단위는 '단어 벡터'입니다.\n",
    "\n",
    "![Alt text](image-9.png)\n",
    "\n",
    "예를 들어 하나의 입력에 대해서 여러개의 출력을 의미하는 일 대 다(one-to-many) 구조의 모델은 하나의 이미지 입력에 대해서 사진의 제목을 출력하는 이미지 캡셔닝 작업에 사용될 수 있습니다. 사진의 제목은 단어들의 나열이므로 시퀀스 출력입니다.\n",
    "\n",
    "하나의 입력에 대해서 여러개의 출력을 의미하는 일 대 다(one-to-many) 구조의 모델은 입력 문서가 긍정적인지 부정적인지를 판별하는 감성 분류(sentiment classification), 또는 메일이 정상 메일인지 스팸 메일인지 판별하는 스팸 메일 분류(spam detection) 등에 사용할 수 있습니다.\n",
    "\n",
    "다 대 다(many-to-many) 구조의 모델의 경우에는 사용자가 문장을 입력하면 대답 문장을 출력하는 챗봇과 입력 문장으로부터 번역된 문장을 출력하는 번역기 등이 이 작업에 속합니다.\n",
    "\n",
    "RNN에 대한 수식을 정의해 보겠습니다.\n",
    "\n",
    "![Alt text](image-10.png)\n",
    "\n",
    "![Alt text](image-11.png)\n",
    "\n",
    "출력층은 결과값인 yt를 계산하기 윟나 활성화 함수로는 푸는 문제에 따라서 다릅니다.\n",
    "예를 들어서 이진 분류를 해야하는 경우라면 출력층에 로지스틱 회귀를 사용하여 시그모이드 함수를 사용할 수 있고 다중 클래스 분류를 해야하는 경우라면 출력층에 소프트맥스 회귀를 사용하여야 하기 때문에 소프트맥스 함수를 사용할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 케라스(Keras)로 RNN 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden_units = 은닉 상태의 크기를 정의. 메모리 셀이 다음 시점의 메모리 셀과 출력층으로 보내는 값의 크기(output_dim)와도 동일. RNN의 용량(capacity)을 늘린다고 보면 되며, 중소형 모델의 경우 보통 128, 256, 512, 1024 등의 값을 가진다.\n",
    "\n",
    "timesteps = 입력 시퀀스의 길이(input_length)라고 표현하기도 함. 시점의 수.\n",
    "\n",
    "input_dim = 입력의 크기.\n",
    "\n",
    "![Alt text](image-12.png)\n",
    "\n",
    "메모리 셀의 최종 시점의 은닉 상태만을 리턴하고자 한다면 (batch_size, timesteps, output_dim)의 은닉 상태값들을 모아서 전체 시퀀스를 리턴하고자 한다면 (batch_szie, timesteps, output_dim) 크기의 3D 텐서를 리턴합니다.\n",
    "\n",
    "마지막 은닉 상태만 전달하도록 하면 다 대 일(many-to-one) 문제를 풀 수 있고, 모든 시점의 은닉 상태를 전달하도록 하면, 다음층에 RNN 은닉층이 하나 더 있는 경우이거나 다 대 다(many-to-many) 문제를 풀 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_5 (SimpleRNN)    (None, 3)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42 (168.00 Byte)\n",
      "Trainable params: 42 (168.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 케라스로 RNN 층을 추가하는 코드는 다음과 같습니다.\n",
    "\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# 모델 객체를 만듭니다.\n",
    "model = Sequential()\n",
    "\n",
    "# RNN 층을 추가합니다.\n",
    "# 은닉 상태의 크기를 정의, 메모리 셀이 다음 시점의 메모리\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, input_shape=(2,10)))\n",
    "# model.add(SimpleRNN(3, input_length=2, input_dim=10))와 동일함.\n",
    "model.summary()\n",
    "\n",
    "# 이 경우 batch_size를 현 단계에서는 알 수 없으므로 (None, 3)이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_6 (SimpleRNN)    (8, 3)                    42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42 (168.00 Byte)\n",
      "Trainable params: 42 (168.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 이번에는 batch_size를 미리 정의해보겠습니다.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, batch_input_shape=(8,2,10)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_7 (SimpleRNN)    (8, 2, 3)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42 (168.00 Byte)\n",
      "Trainable params: 42 (168.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# return_sequences 매개 변수에 True를 기재하여 출력값으로 (batch_size, timesteps, output_dim)\n",
    "# 크기의 3D 텐서를 리턴하도록 모델을 만들어 보겠습니다.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, batch_input_shape=(8,2,10), return_sequences=True))\n",
    "model.summary()\n",
    "\n",
    "# Output Shape이 3D Tensor가 되는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 파이썬으로 RNN 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직접 Numpy로 RNN 층을 구현해보겠습니다. 앞서 메모리 셀에서 은닉 상태를 계산하는 식을 다음과 같이 정의헀습니다.\n",
    "![\n",
    "](image-13.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
